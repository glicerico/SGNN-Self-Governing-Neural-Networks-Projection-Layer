{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Governing Neural Networks (SGNN): the Projection Layer\n",
    "\n",
    "> A SGNN's word projections preprocessing pipeline in scikit-learn\n",
    "\n",
    "In this notebook, we'll use T=80 random hashing projection functions, each of dimensionnality d=14, for a total of 1120 features per projected word in the projection function P. \n",
    "\n",
    "Next, we'll need feedforward neural network (dense) layers on top of that (as in the paper) to re-encode the projection into something better. This is not done in the current notebook and is left to you to implement in your own neural network to train the dense layers jointly with a learning objective. The SGNN projection created hereby is therefore only a preprocessing on the text to project words into the hashing space, which becomes spase 1120-dimensional word features created dynamically hereby. Only the CountVectorizer needs to be fitted, as it is a char n-gram term frequency prior to the hasher. This one could be computed dynamically too without any fit, as it would be possible to use the [power set](https://en.wikipedia.org/wiki/Power_set) of the possible n-grams as sparse indices computed on the fly as (indices, count_value) tuples, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from collections import Counter\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dummy data for demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "[\"Have you ever been in the situation where you've got Jupyter notebooks \"\n",
      " '(iPython notebooks) so huge that you were feeling stuck in your code?',\n",
      " 'Or even worse: have you ever found yourself duplicating your notebook to do '\n",
      " 'changes, and then ending up with lots of badly named notebooks?',\n",
      " \"Well, we've all been here if using notebooks long enough.\",\n",
      " 'So how should we code with notebooks?',\n",
      " \"First, let's see why we need to be careful with notebooks.\",\n",
      " \"Then, let's see how to do TDD inside notebook cells and how to grow a neat \"\n",
      " 'software architecture out of your notebooks.']\n"
     ]
    }
   ],
   "source": [
    "class SentenceTokenizer(BaseEstimator, TransformerMixin):\n",
    "    # char lengths:\n",
    "    MINIMUM_SENTENCE_LENGTH = 10\n",
    "    MAXIMUM_SENTENCE_LENGTH = 200\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self._split(X)\n",
    "    \n",
    "    def _split(self, string_):\n",
    "        splitted_string = []\n",
    "        \n",
    "        sep = chr(29)  # special separator character to split sentences or phrases.\n",
    "        string_ = string_.strip().replace(\".\", \".\" + sep).replace(\"?\", \"?\" + sep).replace(\"!\", \"!\" + sep).replace(\";\", \";\" + sep).replace(\"\\n\", \"\\n\" + sep)\n",
    "        for phrase in string_.split(sep):\n",
    "            phrase = phrase.strip()\n",
    "            \n",
    "            while len(phrase) > SentenceTokenizer.MAXIMUM_SENTENCE_LENGTH:\n",
    "                # clip too long sentences.\n",
    "                sub_phrase = phrase[:SentenceTokenizer.MAXIMUM_SENTENCE_LENGTH].lstrip()\n",
    "                splitted_string.append(sub_phrase)\n",
    "                phrase = phrase[SentenceTokenizer.MAXIMUM_SENTENCE_LENGTH:].rstrip()\n",
    "            \n",
    "            if len(phrase) >= SentenceTokenizer.MINIMUM_SENTENCE_LENGTH:\n",
    "                splitted_string.append(phrase)\n",
    "\n",
    "        return splitted_string\n",
    "\n",
    "\n",
    "with open(\"./data/How-to-Grow-Neat-Software-Architecture-out-of-Jupyter-Notebooks.md\") as f:\n",
    "    raw_data = f.read()\n",
    "\n",
    "test_str_tokenized = SentenceTokenizer().fit_transform(raw_data)\n",
    "\n",
    "# Print text example:\n",
    "print(len(test_str_tokenized))\n",
    "pprint(test_str_tokenized[3:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a SGNN preprocessing pipeline's classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordTokenizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        begin_of_word = \"<\"\n",
    "        end_of_word = \">\"\n",
    "        out = [\n",
    "            [\n",
    "                begin_of_word + word + end_of_word\n",
    "                for word in sentence.replace(\"//\", \" /\").replace(\"/\", \" /\").replace(\"-\", \" -\").replace(\"  \", \" \").split(\" \")\n",
    "                if not len(word) == 0\n",
    "            ]\n",
    "            for sentence in X\n",
    "        ]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_ngram_range = (1, 4)\n",
    "\n",
    "char_term_frequency_params = {\n",
    "    'char_term_frequency__analyzer': 'char',\n",
    "    'char_term_frequency__lowercase': False,\n",
    "    'char_term_frequency__ngram_range': char_ngram_range,\n",
    "    'char_term_frequency__strip_accents': None,\n",
    "    'char_term_frequency__min_df': 2,\n",
    "    'char_term_frequency__max_df': 0.99,\n",
    "    'char_term_frequency__max_features': int(1e7),\n",
    "}\n",
    "\n",
    "class CountVectorizer3D(CountVectorizer):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_flattened_2D = sum(X.copy(), [])\n",
    "        super(CountVectorizer3D, self).fit_transform(X_flattened_2D, y)  # can't simply call \"fit\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [\n",
    "            super(CountVectorizer3D, self).transform(x_2D)\n",
    "            for x_2D in X\n",
    "        ]\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "T = 80\n",
    "d = 14\n",
    "\n",
    "hashing_feature_union_params = {\n",
    "    # T=80 projections for each of dimension d=14: 80 * 14 = 1120-dimensionnal word projections.\n",
    "    **{'union__sparse_random_projection_hasher_{}__n_components'.format(t): d\n",
    "       for t in range(T)\n",
    "    },\n",
    "    **{'union__sparse_random_projection_hasher_{}__dense_output'.format(t): False  # only AFTER hashing.\n",
    "       for t in range(T)\n",
    "    }\n",
    "}\n",
    "\n",
    "class FeatureUnion3D(FeatureUnion):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        X_flattened_2D = sp.vstack(X, format='csr')\n",
    "        super(FeatureUnion3D, self).fit(X_flattened_2D, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X): \n",
    "        return [\n",
    "            super(FeatureUnion3D, self).transform(x_2D)\n",
    "            for x_2D in X\n",
    "        ]\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the pipeline \n",
    "\n",
    "Note: at fit time, the only thing done is to discard some unused char n-grams and to instanciate the random hash, the whole thing could be independent of the data, but here because of discarding the n-grams, we need to \"fit\" the data. Therefore, fitting could be avoided all along, but we fit here for simplicity of implementation using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168 168\n",
      "(12, 1120)\n"
     ]
    }
   ],
   "source": [
    "params = dict()\n",
    "params.update(char_term_frequency_params)\n",
    "params.update(hashing_feature_union_params)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"word_tokenizer\", WordTokenizer()),\n",
    "    (\"char_term_frequency\", CountVectorizer3D()),\n",
    "    ('union', FeatureUnion3D([\n",
    "        ('sparse_random_projection_hasher_{}'.format(t), SparseRandomProjection())\n",
    "        for t in range(T)\n",
    "    ]))\n",
    "])\n",
    "pipeline.set_params(**params)\n",
    "\n",
    "result = pipeline.fit_transform(test_str_tokenized)\n",
    "\n",
    "print(len(result), len(test_str_tokenized))\n",
    "print(result[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see the output and its form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 1120)\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.005715251142432, 0.0, 2.005715251142432, 0.0, 0.0, 2.005715251142432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "{0.0, 2.005715251142432, -2.005715251142432}\n",
      "Counter({0.0: 1069, -2.005715251142432: 27, 2.005715251142432: 24})\n"
     ]
    }
   ],
   "source": [
    "print(result[0].toarray().shape)\n",
    "print(result[0].toarray()[0].tolist())\n",
    "print(\"\")\n",
    "\n",
    "# The whole thing is quite discrete:\n",
    "print(set(result[0].toarray()[0].tolist()))\n",
    "\n",
    "# We see that we could optimize by using integers here instead of floats by counting the occurence of every entry.\n",
    "print(Counter(result[0].toarray()[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking that the cosine similarity before and after word projection is kept\n",
    "\n",
    "Note that this is a yet low-quality test, as the neural network layers above the projection are absent, so the similary is not yet semantic, it only looks at characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word pair tested: ['start', 'started']\n",
      "\t - similarity before: 0.8728715609439697 \t Are words similar? yes\n",
      "\t - similarity after : 0.8542062410985866 \t Are words similar? yes\n",
      "\n",
      "Word pair tested: ['prioritize', 'priority']\n",
      "\t - similarity before: 0.8458888522202895 \t Are words similar? yes\n",
      "\t - similarity after : 0.8495862181305898 \t Are words similar? yes\n",
      "\n",
      "Word pair tested: ['twitter', 'tweet']\n",
      "\t - similarity before: 0.5439282932204212 \t Are words similar? yes\n",
      "\t - similarity after : 0.4826046482460216 \t Are words similar? no\n",
      "\n",
      "Word pair tested: ['Great', 'great']\n",
      "\t - similarity before: 0.8006407690254358 \t Are words similar? yes\n",
      "\t - similarity after : 0.8175049752615363 \t Are words similar? yes\n",
      "\n",
      "Word pair tested: ['boat', 'cow']\n",
      "\t - similarity before: 0.1690308509457033 \t Are words similar? no\n",
      "\t - similarity after : 0.10236537810666581 \t Are words similar? no\n",
      "\n",
      "Word pair tested: ['orange', 'chewbacca']\n",
      "\t - similarity before: 0.14907119849998599 \t Are words similar? no\n",
      "\t - similarity after : 0.2019908169580899 \t Are words similar? no\n",
      "\n",
      "Word pair tested: ['twitter', 'coffee']\n",
      "\t - similarity before: 0.09513029883089882 \t Are words similar? no\n",
      "\t - similarity after : 0.1016460166230715 \t Are words similar? no\n",
      "\n",
      "Word pair tested: ['ab', 'ae']\n",
      "\t - similarity before: 0.408248290463863 \t Are words similar? no\n",
      "\t - similarity after : 0.42850530886130067 \t Are words similar? no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_pairs_to_check_against_each_other = [\n",
    "    # Similar:\n",
    "    [\"start\", \"started\"],\n",
    "    [\"prioritize\", \"priority\"],\n",
    "    [\"twitter\", \"tweet\"],\n",
    "    [\"Great\", \"great\"],\n",
    "    # Dissimilar:\n",
    "    [\"boat\", \"cow\"],\n",
    "    [\"orange\", \"chewbacca\"],\n",
    "    [\"twitter\", \"coffee\"],\n",
    "    [\"ab\", \"ae\"],\n",
    "]\n",
    "\n",
    "before = pipeline.named_steps[\"char_term_frequency\"].transform(word_pairs_to_check_against_each_other)\n",
    "after = pipeline.named_steps[\"union\"].transform(before)\n",
    "\n",
    "for i, word_pair in enumerate(word_pairs_to_check_against_each_other):\n",
    "    cos_sim_before = cosine_similarity(before[i][0], before[i][1])[0,0]\n",
    "    cos_sim_after  = cosine_similarity( after[i][0],  after[i][1])[0,0]\n",
    "    print(\"Word pair tested:\", word_pair)\n",
    "    print(\"\\t - similarity before:\", cos_sim_before, \n",
    "          \"\\t Are words similar?\", \"yes\" if cos_sim_before > 0.5 else \"no\")\n",
    "    print(\"\\t - similarity after :\", cos_sim_after , \n",
    "          \"\\t Are words similar?\", \"yes\" if cos_sim_after  > 0.5 else \"no\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next up\n",
    "\n",
    "So we have created the sentence preprocessing pipeline and the sparse projection (random hashing) function. We now need a few feedforward layers on top of that. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
