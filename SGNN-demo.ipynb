{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Governing Neural Networks (SGNN): the Projection Layer\n",
    "\n",
    "> A SGNN's word projections preprocessing pipeline in scikit-learn\n",
    "\n",
    "In this notebook, we'll use T=80 random hashing projection functions, each of dimensionnality d=14, for a total of 1120 features per projected word in the projection function P. \n",
    "\n",
    "Next, we'll need feedforward neural network (dense) layers on top of that (as in the paper) to re-encode the projection into something better. This is not done in the current notebook and is left to you to implement in your own neural network to train the dense layers jointly with a learning objective. The SGNN projection created hereby is therefore only a preprocessing on the text to project words into the hashing space, which becomes spase 1120-dimensional word features created dynamically hereby. Only the CountVectorizer needs to be fitted, as it is a char n-gram term frequency prior to the hasher. This one could be computed dynamically too without any fit, as it would be possible to use the [power set](https://en.wikipedia.org/wiki/Power_set) of the possible n-grams as sparse indices computed on the fly as (indices, count_value) tuples, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dummy data for demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's easier to start a project by using #Jupyter #Notebooks, but you must move quickly to grow #SoftwareArchitecture out of the notebook before it gets too big\",\n",
       " \"Keeping #CleanCode 's rules of thumbs in mind *does help*\",\n",
       " 'But prioritize #FirstPrinciples over rules of thumb',\n",
       " 'https://twitter',\n",
       " 'com/guillaume_che/status/1075891355866550274']"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SentenceTokenizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [\n",
    "            [r.strip() for r in some_paragraph_or_text_block.split(\".\")]\n",
    "            for some_paragraph_or_text_block in X\n",
    "        ]\n",
    "\n",
    "test_str_tokenized = SentenceTokenizer().fit_transform([\n",
    "    \"It's easier to start a project by using #Jupyter #Notebooks, \" +\n",
    "    \"but you must move quickly to grow #SoftwareArchitecture out of the notebook before it gets too big. \" +\n",
    "    \"Keeping #CleanCode 's rules of thumbs in mind *does help*. \" +\n",
    "    \"But prioritize #FirstPrinciples over rules of thumb. \" +\n",
    "    \"https://twitter.com/guillaume_che/status/1075891355866550274\"\n",
    "])[0]\n",
    "\n",
    "test_str_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a SGNN preprocessing pipeline's classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordTokenizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        begin_of_word = \"<\"\n",
    "        end_of_word = \">\"\n",
    "        out = [\n",
    "            [\n",
    "                begin_of_word + word + end_of_word\n",
    "                for word in sentence.replace(\"//\", \" /\").replace(\"/\", \" /\").replace(\"-\", \" -\").replace(\"  \", \" \").split(\" \")\n",
    "                if not len(word) == 0\n",
    "            ]\n",
    "            for sentence in X\n",
    "        ]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_ngram_range = (1, 4)\n",
    "\n",
    "char_term_frequency_params = {\n",
    "    'char_term_frequency__analyzer': 'char',\n",
    "    'char_term_frequency__lowercase': False,\n",
    "    'char_term_frequency__ngram_range': char_ngram_range,\n",
    "    'char_term_frequency__strip_accents': None,\n",
    "    'char_term_frequency__min_df': 2,\n",
    "    'char_term_frequency__max_df': 0.99,\n",
    "    'char_term_frequency__max_features': int(1e7),\n",
    "}\n",
    "\n",
    "class CountVectorizer3D(CountVectorizer):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_flattened_2D = sum(X.copy(), [])\n",
    "        super(CountVectorizer3D, self).fit_transform(X_flattened_2D, y)  # can't simply call \"fit\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [\n",
    "            super(CountVectorizer3D, self).transform(x_2D)\n",
    "            for x_2D in X\n",
    "        ]\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "T = 80\n",
    "d = 14\n",
    "\n",
    "hashing_feature_union_params = {\n",
    "    # T=80 projections for each of dimension d=14: 80 * 14 = 1120-dimensionnal word projections.\n",
    "    **{'union__sparse_random_projection_hasher_{}__n_components'.format(t): d\n",
    "       for t in range(T)\n",
    "    },\n",
    "    **{'union__sparse_random_projection_hasher_{}__dense_output'.format(t): False  # only AFTER hashing.\n",
    "       for t in range(T)\n",
    "    }\n",
    "}\n",
    "\n",
    "class FeatureUnion3D(FeatureUnion):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        X_flattened_2D = sp.vstack(X, format='csr')\n",
    "        super(FeatureUnion3D, self).fit(X_flattened_2D, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X): \n",
    "        return [\n",
    "            super(FeatureUnion3D, self).transform(x_2D)\n",
    "            for x_2D in X\n",
    "        ]\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the pipeline \n",
    "\n",
    "Note: at fit time, the only thing done is to discard some unused char n-grams and to instanciate the random hash, the whole thing could be independent of the data, but here because of discarding the n-grams, we need to \"fit\" the data. Therefore, fitting could be avoided all along, but we fit here for simplicity of implementation using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('word_tokenizer', WordTokenizer()), ('char_term_frequency', CountVectorizer3D(analyzer='char', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=False, max_df=0.99, max_features=10000000, min_df=2,\n",
       "         ngram_...to', eps=0.1,\n",
       "            n_components=14, random_state=None))],\n",
       "        transformer_weights=None))])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = dict()\n",
    "params.update(char_term_frequency_params)\n",
    "params.update(hashing_feature_union_params)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"word_tokenizer\", WordTokenizer()),\n",
    "    (\"char_term_frequency\", CountVectorizer3D()),\n",
    "    ('union', FeatureUnion3D([\n",
    "        ('sparse_random_projection_hasher_{}'.format(t), SparseRandomProjection())\n",
    "        for t in range(T)\n",
    "    ]))\n",
    "])\n",
    "pipeline.set_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<27x1120 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 14003 stored elements in Compressed Sparse Row format>,\n",
       " <10x1120 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 5269 stored elements in Compressed Sparse Row format>,\n",
       " <7x1120 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 4436 stored elements in Compressed Sparse Row format>,\n",
       " <2x1120 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1159 stored elements in Compressed Sparse Row format>,\n",
       " <4x1120 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1832 stored elements in Compressed Sparse Row format>]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pipeline.fit_transform(test_str_tokenized)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see some statistics of the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 1120)\n",
      "[0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, -0.9093104492176721, 0.9093104492176721, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.0, 0.9093104492176721, 0.0, 0.0, 0.9093104492176721, 0.0, 0.9093104492176721, 0.0, -0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.9093104492176721, 0.0, 0.9093104492176721, -0.9093104492176721, -1.8186208984353442, 0.0, 0.0, 0.0, 0.9093104492176721, -0.9093104492176721, 0.0, 0.0, -1.8186208984353442, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, -0.9093104492176721, -0.9093104492176721, 0.9093104492176721, -1.8186208984353442, 0.0, 0.0, 0.0, 1.8186208984353442, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, -0.9093104492176721, -0.9093104492176721, 0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.9093104492176721, -0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.9093104492176721, -0.9093104492176721, -0.9093104492176721, 0.9093104492176721, 0.0, 0.0, 0.0, 1.8186208984353442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, -0.9093104492176721, 0.9093104492176721, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, -0.9093104492176721, 0.9093104492176721, 0.0, -0.9093104492176721, 0.9093104492176721, -0.9093104492176721, -0.9093104492176721, 0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 1.8186208984353442, 0.0, 0.9093104492176721, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, -0.9093104492176721, 1.8186208984353442, 0.0, -1.8186208984353442, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, -1.8186208984353442, 0.0, 1.8186208984353442, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.9093104492176721, -0.9093104492176721, 0.9093104492176721, 0.0, 0.9093104492176721, 0.9093104492176721, 0.0, -0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.9093104492176721, 0.0, -0.9093104492176721, -0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.9093104492176721, 0.9093104492176721, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, -1.8186208984353442, 0.0, 0.9093104492176721, 0.0, -0.9093104492176721, 0.9093104492176721, 0.0, 0.0, 0.0, 0.9093104492176721, 0.9093104492176721, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.9093104492176721, 0.9093104492176721, 0.0, 0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 1.8186208984353442, -1.8186208984353442, 1.8186208984353442, 0.0, 0.9093104492176721, 0.0, 1.8186208984353442, 0.9093104492176721, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.9093104492176721, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, -1.8186208984353442, 0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.9093104492176721, 0.9093104492176721, 0.0, -0.9093104492176721, -1.8186208984353442, -2.7279313476530165, 0.0, -0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.9093104492176721, 0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.9093104492176721, 0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.9093104492176721, -0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 1.8186208984353442, 0.0, -0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.9093104492176721, 0.0, 0.0, 0.0, -0.9093104492176721, -1.8186208984353442, 0.0, 0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, -0.9093104492176721, -0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.9093104492176721, -0.9093104492176721, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, -1.8186208984353442, 0.0, -0.9093104492176721, 0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 1.8186208984353442, 0.0, 1.8186208984353442, 0.0, -0.9093104492176721, -1.8186208984353442, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.8186208984353442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, -0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.0, -0.9093104492176721, -0.9093104492176721, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, -1.8186208984353442, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.9093104492176721, 0.9093104492176721, 0.0, 0.9093104492176721, 0.0, -0.9093104492176721, 0.9093104492176721, 0.9093104492176721, 0.0, -0.9093104492176721, 0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.9093104492176721, -0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, -0.9093104492176721, 0.9093104492176721, 0.0, -1.8186208984353442, 0.9093104492176721, 0.9093104492176721, -1.8186208984353442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, -1.8186208984353442, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.9093104492176721, 0.9093104492176721, -0.9093104492176721, -0.9093104492176721, 0.0, 1.8186208984353442, 0.0, 0.0, 0.0, 0.9093104492176721, 0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 1.8186208984353442, -0.9093104492176721, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, -1.8186208984353442, 1.8186208984353442, 0.9093104492176721, 0.0, 0.0, 0.9093104492176721, 0.9093104492176721, 0.0, -1.8186208984353442, 0.0, -1.8186208984353442, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.9093104492176721, 0.9093104492176721, -0.9093104492176721, -0.9093104492176721, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.9093104492176721, -0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, -0.9093104492176721, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, -0.9093104492176721, 0.9093104492176721, 0.9093104492176721, 0.9093104492176721, 0.0, 0.9093104492176721, 0.0, 0.0, 0.9093104492176721, -0.9093104492176721, -0.9093104492176721, 0.0, -0.9093104492176721, 0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.0, 0.9093104492176721, 0.9093104492176721, 0.0, 0.0, 0.9093104492176721, -0.9093104492176721, -0.9093104492176721, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.9093104492176721, 0.0, 1.8186208984353442, 1.8186208984353442, 0.9093104492176721, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, -0.9093104492176721, -0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.9093104492176721, -0.9093104492176721, 0.9093104492176721, -0.9093104492176721, -0.9093104492176721, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.9093104492176721, 0.9093104492176721, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, -1.8186208984353442, 0.0, 0.0, 0.0, 0.9093104492176721, 0.9093104492176721, -0.9093104492176721, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, -0.9093104492176721, 0.0, 0.9093104492176721, 0.0, 0.9093104492176721, 0.9093104492176721, 0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, -0.9093104492176721, 0.9093104492176721, -0.9093104492176721, -0.9093104492176721, 0.0, 0.0, 0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.9093104492176721, 0.0, -1.8186208984353442, -0.9093104492176721, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 1.8186208984353442, 0.0, 0.0, 0.9093104492176721, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.8186208984353442, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, -0.9093104492176721, -0.9093104492176721, -0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, 0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 1.8186208984353442, 0.0, 0.0, 0.0, 0.9093104492176721, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, -0.9093104492176721, 0.0, -0.9093104492176721, 0.0, -1.8186208984353442, 0.9093104492176721, 0.9093104492176721, 0.9093104492176721, -0.9093104492176721, 0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, -1.8186208984353442, 0.0, 0.0, -2.7279313476530165, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, -0.9093104492176721, -0.9093104492176721, -0.9093104492176721, 1.8186208984353442, -0.9093104492176721, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9093104492176721, -0.9093104492176721, 0.9093104492176721, 0.0, 0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, -1.8186208984353442, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.9093104492176721, 0.0, 0.0, -0.9093104492176721, -0.9093104492176721, 0.0, -0.9093104492176721, 0.9093104492176721, 0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9093104492176721, 0.0, 0.0, 0.9093104492176721, 1.8186208984353442, 0.0, 0.9093104492176721, 0.9093104492176721, 0.9093104492176721, 0.0, 0.0, -0.9093104492176721, 0.0, 0.9093104492176721, -0.9093104492176721, 0.0, -0.9093104492176721, 0.0, 0.0, 0.9093104492176721, 1.8186208984353442, -0.9093104492176721]\n",
      "\n",
      "{0.0, -0.9093104492176721, 0.9093104492176721, 1.8186208984353442, -2.7279313476530165, -1.8186208984353442}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 716,\n",
       "         -0.9093104492176721: 178,\n",
       "         0.9093104492176721: 177,\n",
       "         -1.8186208984353442: 25,\n",
       "         1.8186208984353442: 22,\n",
       "         -2.7279313476530165: 2})"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(result[0].toarray().shape)\n",
    "print(result[0].toarray()[0].tolist())\n",
    "print(\"\")\n",
    "\n",
    "# The whole thing is quite discrete:\n",
    "print(set(result[0].toarray()[0].tolist()))\n",
    "\n",
    "# We see that we could optimize by using integers here instead of floats by counting the occurence of every entry.\n",
    "Counter(result[0].toarray()[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next up\n",
    "\n",
    "So we have created the sentence preprocessing pipeline and the sparse projection (random hashing) function. We now need a few feedforward layers on top of that. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
